source("c:/alexey_workspace/sauder/kaggle/codes-in-r/GBM_20140120.r")
View(gbm_function)
View(gbm_f)
View(gbm_f)
View(gbm_f)
1+1
sigma * sqrt(dt) * rnorm(n = 1))
View(gbm_function)
}
# source("c:/alexey_workspace/sauder/kaggle/data-science-titanic-learn/code/code.R")
fnScalingUnset = function(z, mu, sigma){
return (z*sigma + mu)
}
fnScaleData = function(z, bExcludeNA, mu = NA, sigma = NA){
if(is.na(mu) | is.na(sigma))
{
z.scaled = (z - mean(z, na.rm = bExcludeNA))/sd(z, na.rm = bExcludeNA)
}
else
{
z.scaled = (z - mu)/sigma
}
return (z.scaled)
}
fnSplitTrainingValidation = function(dataset){
n = dim(rawdata)[1]
dataset.training	= rawdata[1:floor(n*2/3),]
# dataset.validation	= rawdata[1+floor(n*2/3):(n-1),]
return (dataset.training)
}
# ctrl + L
# clear all variables
rm(list = setdiff(ls(), lsf.str()))
strDir	= "c:/alexey_workspace/sauder/kaggle/data-science-titanic-learn/"
strData	= paste(strDir,"data/", sep = "")
strOut	= paste(strDir,"out/", sep = "")
# set current working directory
setwd(strDir)
getwd()
# read data from the input file
strFileIn	= paste(strData,"train/train.csv", sep = "")
strFileOut	= paste(strOut,"out.csv", sep = "")
strLog		= paste(strOut,"out-log.csv", sep = "")
rawdata <- read.table(strFileIn, header = TRUE, sep = ",")
training	= fnSplitTrainingValidation(rawdata)
summary(training)
bExcludeNA = TRUE
y = as.factor(training$Survived)
x1 = training$Age
x1.scaled = fnScaleData(x, bExcludeNA)
# x1_ = fnScalingUnset(x1.scaled,mean(x1, na.rm = bExcludeNA),sd(x1, na.rm = bExcludeNA))
x2 = as.factor(training$Sex)
# source("c:/alexey_workspace/sauder/kaggle/data-science-titanic-learn/code/code.R")
fnScalingUnset = function(z, mu, sigma){
return (z*sigma + mu)
}
fnScaleData = function(z, bExcludeNA, mu = NA, sigma = NA){
if(is.na(mu) | is.na(sigma))
{
z.scaled = (z - mean(z, na.rm = bExcludeNA))/sd(z, na.rm = bExcludeNA)
}
else
{
z.scaled = (z - mu)/sigma
}
return (z.scaled)
}
fnSplitTrainingValidation = function(dataset){
n = dim(rawdata)[1]
dataset.training	= rawdata[1:floor(n*2/3),]
# dataset.validation	= rawdata[1+floor(n*2/3):(n-1),]
return (dataset.training)
}
# ctrl + L
# clear all variables
rm(list = setdiff(ls(), lsf.str()))
strDir	= "c:/alexey_workspace/sauder/kaggle/data-science-titanic-learn/"
strData	= paste(strDir,"data/", sep = "")
strOut	= paste(strDir,"out/", sep = "")
# set current working directory
setwd(strDir)
getwd()
# read data from the input file
strFileIn	= paste(strData,"train/train.csv", sep = "")
strFileOut	= paste(strOut,"out.csv", sep = "")
strLog		= paste(strOut,"out-log.csv", sep = "")
rawdata <- read.table(strFileIn, header = TRUE, sep = ",")
training	= fnSplitTrainingValidation(rawdata)
summary(training)
bExcludeNA = TRUE
y = as.factor(training$Survived)
x1 = training$Age
x1.scaled = fnScaleData(x1, bExcludeNA)
# x1_ = fnScalingUnset(x1.scaled,mean(x1, na.rm = bExcludeNA),sd(x1, na.rm = bExcludeNA))
x1_ = fnScalingUnset(x1.scaled,mean(x1, na.rm = bExcludeNA),sd(x1, na.rm = bExcludeNA))
x1_ -x1
length(x1_-x1)
x3 = training$Fare
plot(x3,y,xlab = "passenger age", ylab = "survived no/yes")
mymodel.fit = glm(y ~ x3,family = binomial(link = logit), data = training)
summary(mymodel.fit)
x3 = training$Fare
x3.scaled = fnScaleData(x3, bExcludeNA)
mymodel.fit = glm(y ~ x3.scaled,family = binomial(link = logit), data = training)
summary(mymodel.fit)
cor(x1.scaled)
cor(x1.scaled,x3.scaled)
cor(x1.scaled,x1.scaled)
cor(x3.scaled,x3.scaled)
cor(x1.scaled,x3.scaled)
pairs(cbind(y,x1.scaled), lower.panel = panel.cor)
pairs(cbind(y,x1.scaled), lower.panel = fnOutCorrPanel)
fnOutCorrPanel = function(x,y, digits = 2, prefix = ""){
usr = par("usr"); on.exit(par(usr))
par(usr = c(0,1,0,1))
r = cor(x,y)
txt = format(c(r,0.123456789),digits =digits)[1]
txt = paste(prefix, txt, sep = "")
text(.5,.5,txt)
}
pairs(cbind(y,x1.scaled), lower.panel = fnOutCorrPanel)
pairs(cbind(y,x1.scaled,x3.scaled), lower.panel = fnOutCorrPanel)
summary(rawdata)
header(rawdata)
rawdata[1,1]
rawdata[0,1]
rawdata[1,:]
rawdata[1,]
rawdata.header
colnames(x)
colnames(rawdata)
colnames(training)
colnames(training)[2]
length(colnames(training))
nN = length(colnames(training))
nN
fnRemoveIrrelevantFactors = function(z){
return (z)
}
training.preprocessed = fnRemoveIrrelevantFactors(training)
namesToRemove = c("PassengerId","Name")
fnRemoveIrrelevantFactors = function(z,namesToRemove){
return (subset(z, select=-cNames))
}
training.preprocessed = fnRemoveIrrelevantFactors(training, namesToRemove)
fnRemoveIrrelevantFactors = function(z,namesToRemove){
return (subset(z, select=-namesToRemove))
}
training.preprocessed = fnRemoveIrrelevantFactors(training, namesToRemove)
help("subset")
subset(training, select=-c("PassengerId","Name"))
c(PassengerId,Name)
c("PassengerId"","Name"")
c("PassengerId","Name"")
c("PassengerId","Name")
c("PassengerId","Name")
training[ , -which(names(training) %in% c("z","u"))]
training[ , -which(names(training) %in% c("PassengerId","Name"))]
summary(training[ , -which(names(training) %in% c("PassengerId","Name"))])
fnRemoveIrrelevantFactors = function(z,namesToRemove){
# summary(training[ , -which(names(training) %in% c("PassengerId","Name"))])
return (z[ , -which(names(z) %in% namesToRemove)])
}
namesToRemove = c("PassengerId","Name")
training.preprocessed = fnRemoveIrrelevantFactors(training, namesToRemove)
# get rid of factors that are for sure irrelevant
# source("c:/alexey_workspace/sauder/kaggle/data-science-titanic-learn/code/code.R")
fnRemoveFactors = function(z,namesToRemove){
return (z[ , -which(names(z) %in% namesToRemove)])
}
fnOutCorrPanel = function(x,y, digits = 2, prefix = ""){
usr = par("usr"); on.exit(par(usr))
par(usr = c(0,1,0,1))
r = cor(x,y)
txt = format(c(r,0.123456789),digits =digits)[1]
txt = paste(prefix, txt, sep = "")
text(.5,.5,txt)
}
fnScalingUnset = function(z, mu, sigma){
return (z*sigma + mu)
}
fnScaleData = function(z, bExcludeNA, mu = NA, sigma = NA){
if(is.na(mu) | is.na(sigma))
{
z.scaled = (z - mean(z, na.rm = bExcludeNA))/sd(z, na.rm = bExcludeNA)
}
else
{
z.scaled = (z - mu)/sigma
}
return (z.scaled)
}
fnSplitTrainingValidation = function(dataset){
n = dim(rawdata)[1]
dataset.training	= rawdata[1:floor(n*2/3),]
# dataset.validation	= rawdata[1+floor(n*2/3):(n-1),]
return (dataset.training)
}
# ctrl + L
# clear all variables
rm(list = setdiff(ls(), lsf.str()))
strDir	= "c:/alexey_workspace/sauder/kaggle/data-science-titanic-learn/"
strData	= paste(strDir,"data/", sep = "")
strOut	= paste(strDir,"out/", sep = "")
# set current working directory
setwd(strDir)
getwd()
# read data from the input file
strFileIn	= paste(strData,"train/train.csv", sep = "")
strFileOut	= paste(strOut,"out.csv", sep = "")
strLog		= paste(strOut,"out-log.csv", sep = "")
rawdata <- read.table(strFileIn, header = TRUE, sep = ",")
training	= fnSplitTrainingValidation(rawdata)
summary(training)
# get rid of factors that are for sure irrelevant
namesToRemove = c("PassengerId","Name")
training.preprocessed = fnRemoveFactors(training, namesToRemove)
# determine number of nFactors
nN = length(colnames(training.preprocessed)) - 1
bExcludeNA = TRUE
y = as.factor(training.preprocessed$Survived)
x1 = training.preprocessed$Age
x1.scaled = fnScaleData(x1, bExcludeNA)
# x1_ = fnScalingUnset(x1.scaled,mean(x1, na.rm = bExcludeNA),sd(x1, na.rm = bExcludeNA))
x2 = as.factor(training.preprocessed$Sex)
x3 = training.preprocessed$Fare
x3.scaled = fnScaleData(x3, bExcludeNA)
pairs(cbind(y,x1.scaled,x3.scaled), lower.panel = fnOutCorrPanel)
cor(training.preprocessed)
